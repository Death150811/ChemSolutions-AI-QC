# ChemSolutions-AI-QC
–†–µ–∞–ª–∏–∑–∞—Ü–∏—è ML-—Å–∏—Å—Ç–µ–º—ã –∫–æ–Ω—Ç—Ä–æ–ª—è –∫–∞—á–µ—Å—Ç–≤–∞ —Ñ–∞—Ä–º–∞—Ü–µ–≤—Ç–∏—á–µ—Å–∫–∏—Ö –≤–µ—â–µ—Å—Ç–≤, –≤–∫–ª—é—á–∞—è –ø—Ä–µ–¥–º–µ—Ç–Ω—É—é –∞–Ω–∞–ª–∏—Ç–∏–∫—É –æ–±–æ—Ä—É–¥–æ–≤–∞–Ω–∏—è –∏ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ —á–∏—Å—Ç–æ—Ç—ã –ø–∞—Ä—Ç–∏–π —Å –ø–æ–º–æ—â—å—é PAT-–∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤

–ù–∏–∂–µ –≥–æ—Ç–æ–≤—ã–π Jupyter Notebook, –ø–æ–ª–Ω–æ—Å—Ç—å—é self-contained, –∫–æ—Ç–æ—Ä—ã–π –º–æ–∂–Ω–æ –ø—Ä–æ—Å—Ç–æ —Å–∫–æ–ø–∏—Ä–æ–≤–∞—Ç—å –≤ —Ñ–∞–π–ª .ipynb –∏ –∑–∞–≥—Ä—É–∑–∏—Ç—å –Ω–∞ GitHub. –û–Ω –≤–∫–ª—é—á–∞–µ—Ç: –ø—Ä–æ–≤–µ—Ä–∫—É –¥–∞–Ω–Ω—ã—Ö, —Å–∏–Ω—Ç–µ—Ç–∏–∫—É, –æ–±—É—á–µ–Ω–∏–µ XGBoost, SHAP-–∞–Ω–∞–ª–∏–∑ –∏ –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—é ¬´—á—Ç–æ –µ—Å–ª–∏¬ª –¥–ª—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π.
–ú–µ—Ö–∞–Ω–∏–∑–º –ê 

---
# ChemSolutions AI-QC Prototype

## üìñ –û–ø–∏—Å–∞–Ω–∏–µ
–î–∞–Ω–Ω—ã–π –ø—Ä–æ—Ç–æ—Ç–∏–ø —Å–æ–∑–¥–∞–Ω –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏ –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏ **AI-Driven Quality Control** –≤ —Ñ–∞—Ä–º–∞—Ü–µ–≤—Ç–∏—á–µ—Å–∫–æ–º –ø—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤–µ.  
–°–∏—Å—Ç–µ–º–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç **–º–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ** –∏ **–∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã PAT (Process Analytical Technology)** –¥–ª—è:
- –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –ø—Ä–æ—Ü–µ—Å—Å–æ–≤ (CPP)
- –ü—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ –ø–∞—Ä—Ç–∏–π
- –°–Ω–∏–∂–µ–Ω–∏—è —Ä–∏—Å–∫–∞ –±—Ä–∞–∫–∞ –∏ –ø—Ä–æ—Å—Ç–æ–µ–≤ –æ–±–æ—Ä—É–¥–æ–≤–∞–Ω–∏—è
- –ü–æ–≤—ã—à–µ–Ω–∏—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –ø—Ä–æ–¥—É–∫—Ü–∏–∏

### –û—Å–Ω–æ–≤–Ω—ã–µ –º–æ–¥—É–ª–∏:
1. **–ö–æ–Ω—Ç—É—Ä 1 ‚Äî –ö–æ–Ω—Ç—Ä–æ–ª—å –∫–∞—á–µ—Å—Ç–≤–∞ —Å—ã—Ä—å—è –∏ –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã—Ö —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–π**  
   –ü—Ä–æ–≥–Ω–æ–∑ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –±—Ä–∞–∫–∞ –µ—â—ë –¥–æ –Ω–∞—á–∞–ª–∞ —Å–∏–Ω—Ç–µ–∑–∞.
2. **–ö–æ–Ω—Ç—É—Ä 2 ‚Äî –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö —Å—Ç–∞–¥–∏–π —Å–∏–Ω—Ç–µ–∑–∞**  
   –û–Ω–ª–∞–π–Ω-–∞–Ω–∞–ª–∏–∑ (pH, —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞, NIR, Raman) –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è –æ—à–∏–±–æ–∫.
3. **–ö–æ–Ω—Ç—É—Ä 3 ‚Äî –ü—Ä–æ–≥–Ω–æ–∑ –∫–∞—á–µ—Å—Ç–≤–∞ –≥–æ—Ç–æ–≤–æ–≥–æ –ø—Ä–æ–¥—É–∫—Ç–∞**  
   –ò—Ç–æ–≥–æ–≤—ã–π –∫–æ–Ω—Ç—Ä–æ–ª—å –ø–∞—Ä—Ç–∏–∏ –∏ –ø—Ä–µ–¥–∏–∫—Ç–∏–≤–Ω–∞—è –∞–Ω–∞–ª–∏—Ç–∏–∫–∞ –≤—ã—Ö–æ–¥–∞.

---
# SmartQC ‚Äî AI –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ –∏ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –∞–Ω–æ–º–∞–ª–∏–π

## üöÄ –ö–∞–∫ –∑–∞–ø—É—Å—Ç–∏—Ç—å
1. –°–∫–æ–ø–∏—Ä—É–π—Ç–µ –ª—é–±–æ–π –∏–∑ –∫–æ–¥–æ–≤—ã—Ö –±–ª–æ–∫–æ–≤ –Ω–∏–∂–µ –≤ Jupyter Notebook –∏–ª–∏ Google Colab.
2. –ó–∞–ø—É—Å—Ç–∏—Ç–µ —è—á–µ–π–∫–∏ –ø–æ –ø–æ—Ä—è–¥–∫—É.
3. –†–µ–∑—É–ª—å—Ç–∞—Ç: –≥—Ä–∞—Ñ–∏–∫–∏ SHAP, ROC AUC –∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –¥–ª—è –æ–ø–µ—Ä–∞—Ç–æ—Ä–æ–≤.

---

## üîπ –ö–æ–Ω—Ç—É—Ä A: –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –∫–∞—á–µ—Å—Ç–≤–∞ (Soft Sensors)

```python
pip install xgboost shap scikit-learn pandas matplotlib

import os
import pandas as pd
import numpy as np
from sklearn.model_selection import TimeSeriesSplit
from sklearn.metrics import roc_auc_score, mean_absolute_error
from xgboost import XGBClassifier
import shap
import matplotlib.pyplot as plt

# --- –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è –¥–∞–Ω–Ω—ã—Ö ---
if os.path.exists("batches_aggregated.csv"):
    df = pd.read_csv("batches_aggregated.csv")
else:
    # —Å–æ–∑–¥–∞—ë–º —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–π –¥–∞—Ç–∞—Å–µ—Ç
    np.random.seed(42)
    n = 800
    df = pd.DataFrame({
        "temp_mean": np.random.normal(72,3,n),
        "pH_mean": np.random.normal(7.2,0.4,n),
        "pressure_mean": np.random.normal(1.2, 0.15,n),
        "dosing_var": np.random.normal(0.02, 0.01, n),
        "torque_max": np.random.normal(40,5,n),
    })
    df["purity"] = 100 - abs(df["temp_mean"]-72)*0.8 - abs(df["pH_mean"]-7.2)*5 - df["dosing_var"]*100
    df["label_good"] = (df["purity"] >= 98).astype(int)

print("–î–∞–Ω–Ω—ã–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã / —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω—ã:")
print(df.head())

# --- –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ train/test (time-aware) ---
train = df.iloc[:640]
test = df.iloc[640:]

features = ["temp_mean","pH_mean","pressure_mean","dosing_var","torque_max"]

# --- –û–±—É—á–µ–Ω–∏–µ XGBoost ---
model = XGBClassifier(
    n_estimators=200,
    max_depth=4,
    learning_rate=0.05,
    random_state=42
)
model.fit(train[features], train["label_good"])
probs = model.predict_proba(test[features])[:,1]

print("\nROC AUC –Ω–∞ —Ç–µ—Å—Ç–µ:", roc_auc_score(test["label_good"], probs))

# --- SHAP Explainability ---
explainer = shap.TreeExplainer(model)
shap_vals = explainer.shap_values(test[features])
shap.summary_plot(shap_vals, test[features], show=False)
plt.savefig("shap_summary.png")  # —Å–æ—Ö—Ä–∞–Ω—è–µ–º –≥—Ä–∞—Ñ–∏–∫ –¥–ª—è –ø—Ä–µ–∑–µ–Ω—Ç–∞—Ü–∏–∏
plt.close()

# --- –ü—Ä–æ—Å—Ç–∞—è recommendation rule (–ª–æ–∫–∞–ª—å–Ω–∞—è —Å–∏–º—É–ª—è—Ü–∏—è) ---
i = test.index[-1]
row = test.loc[i].copy()
base_prob = model.predict_proba(row[features].values.reshape(1,-1))[0,1]

suggestions = []
for feat in ["temp_mean","pH_mean","dosing_var"]:
    modified = row[features].copy()
    if feat == "temp_mean":
        modified["temp_mean"] -= 1.0
    elif feat == "pH_mean":
        modified["pH_mean"] += 0.1
    elif feat == "dosing_var":
        modified["dosing_var"] *= 0.95
    new_prob = model.predict_proba(modified.values.reshape(1,-1))[0,1]
    suggestions.append((feat, base_prob, new_prob, new_prob-base_prob))

print("\nBase prob:", round(base_prob,3))
print("–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ (feature, base_prob, new_prob, delta):")
for s in sorted(suggestions, key=lambda x: -x[3]):
    print(s)

# --- –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è predicted vs real purity ---
plt.scatter(test["purity"], probs*100)
plt.xlabel("Real purity (%)")
plt.ylabel("Predicted P(good) * 100")
plt.title("Predicted vs Real Purity")
plt.savefig("pred_vs_real.png")
plt.show() 












‚àÜ‚àÜ‚àÜ‚àÜ–ö–æ–Ω—Ç—É—Ä B ‚Äî –û–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –∞–Ω–æ–º–∞–ª–∏–π (Anomaly Detection / SPC 2.0) ‚Äî –ø–æ–ª–Ω—ã–π, –ø—Ä–∞–∫—Ç–∏—á–Ω—ã–π —Ä–∞–∑–±–æ—Ä

–ö–æ—Ä–æ—Ç–∫–æ: —Ü–µ–ª—å ‚Äî –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏ –æ–±–Ω–∞—Ä—É–∂–∏–≤–∞—Ç—å –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏—è –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ (–¥–∞—Ç—á–∏–∫–∏, —Å–ø–µ–∫—Ç—Ä—ã, –ø–æ–≤–µ–¥–µ–Ω–∏–µ —Ä–µ–∞–∫—Ç–æ—Ä–∞), –∫–æ—Ç–æ—Ä—ã–µ –ø—Ä–µ–¥—à–µ—Å—Ç–≤—É—é—Ç –±—Ä–∞–∫—É/–Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω–æ–π —á–∏—Å—Ç–æ—Ç–µ –ø–∞—Ä—Ç–∏–∏ –∏–ª–∏ –∞–≤–∞—Ä–∏–∏ –æ–±–æ—Ä—É–¥–æ–≤–∞–Ω–∏—è, –∏ –¥–∞–≤–∞—Ç—å —Ä–∞–Ω–Ω–µ–µ, –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º–æ–µ –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ –æ–ø–µ—Ä–∞—Ç–æ—Ä—É.

–ù–∏–∂–µ ‚Äî –ø–æ–¥—Ä–æ–±–Ω–∞—è ¬´–∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è –¥–æ –≤–∏–Ω—Ç–∏–∫–∞¬ª: —á—Ç–æ –≤—ã –ø–æ–ª—É—á–∏—Ç–µ, –∫–∞–∫–∏–µ –¥–∞–Ω–Ω—ã–µ –Ω—É–∂–Ω—ã, –∫–∞–∫ –º–æ–¥–µ–ª–∏—Ç—å/–≤–∞–ª–∏–¥–∞—Ü–∏—è/–¥–µ–ø–ª–æ–π, –∫–∞–∫–∏–µ –º–µ—Ç—Ä–∏–∫–∏ —Å–º–æ—Ç—Ä–µ—Ç—å, –∫–∞–∫ —ç—Ç–æ –≤–ª–∏—è–µ—Ç –Ω–∞ –±–∏–∑–Ω–µ—Å –∏ –Ω–∞ –∫—Ä–∏—Ç–µ—Ä–∏–∏ –∂—é—Ä–∏. –í –∫–æ–Ω—Ü–µ ‚Äî –≥–æ—Ç–æ–≤—ã–π —Ä–∞–±–æ—á–∏–π —Å–∫—Ä–∏–ø—Ç (–ø—Ä–æ–≤–µ—Ä—è–µ—Ç –Ω–∞–ª–∏—á–∏–µ —Ñ–∞–π–ª–∞, –∏–Ω–∞—á–µ –≥–µ–Ω–µ—Ä–∏—Ç —Å–∏–Ω—Ç–µ—Ç–∏–∫—É)
---

1) –ß—Ç–æ –≤—ã–¥–∞—ë—Ç –∫–æ–Ω—Ç—É—Ä B ‚Äî –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç—ã

–ê–Ω–æ–º–∞–ª–∏–π–Ω—ã–π —Å–∫–æ—Ä (–∞–Ω—Ç–∏-¬´–Ω–æ—Ä–º–∞¬ª): —á–∏—Å–ª–æ –Ω–∞ –∫–∞–∂–¥–æ–º –≤—Ä–µ–º–µ–Ω–Ω–æ–º —à–∞–≥–µ.

–ë–∏–Ω–∞—Ä–Ω—ã–π —Ñ–ª–∞–≥ –∞–Ω–æ–º–∞–ª–∏–∏ (multi-tier: –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ/–∫—Ä–∏—Ç–∏—á–Ω–æ).

–í—Ä–µ–º—è –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è (lead time –¥–æ –ª–∞–±–æ—Ä–∞—Ç–æ—Ä–Ω–æ–≥–æ OOS / –æ—Ç–∫–∞–∑–∞).

–ü—Ä–∏—á–∏–Ω–Ω–∞—è –≤–∫–ª–∞–¥–∫–∞: –∫–∞–∫–∏–µ —Å–µ–Ω—Å–æ—Ä—ã –¥–∞–ª–∏ –Ω–∞–∏–±–æ–ª—å—à–∏–π –≤–∫–ª–∞–¥ –≤ —Ä–µ–∫–æ–º–±–∏–Ω–∞—Ü–∏–æ–Ω–Ω—É—é –æ—à–∏–±–∫—É / —Å–∫–æ—Ä (reconstruction error per sensor, top-k features).

–ê–ª–µ—Ä—Ç-–ø–∞–∫–µ—Ç: –≤—Ä–µ–º–µ–Ω–Ω–æ–π —É—á–∞—Å—Ç–æ–∫, —Å–∫–æ—Ä—ã, —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ–º–æ–µ –¥–µ–π—Å—Ç–≤–∏–µ (verify sampler / take corrective action), —É–Ω–∏–∫–∞–ª—å–Ω—ã–π id –∏–Ω—Ü–∏–¥–µ–Ω—Ç–∞ –¥–ª—è —Ç—Ä–µ–∫–∏–Ω–≥–∞.

–î–∞—à–±–æ—Ä–¥-–≤–∏–¥–∂–µ—Ç—ã: timeline —Å –æ—Ç–º–µ—Ç–∫–∞–º–∏ –∞–Ω–æ–º–∞–ª–∏–π; —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å–∫–æ—Äo–≤; recent incidents list.



---

2) –ö–∞–∫–∏–µ –¥–∞–Ω–Ω—ã–µ –Ω—É–∂–Ω—ã (–∏ –∫–∞–∫ –∏—Ö –ø–æ–ª—É—á–∏—Ç—å)

SCADA / DCS (–æ–Ω–ª–∞–π–Ω): T, pH, P, flow rates, torque, power, –Ω–∞—Å–æ—Å–Ω—ã–µ —Ä–∞—Å—Ö–æ–¥/—Å–∫–æ—Ä–æ—Å—Ç—å ‚Äî —á–∞—Å—Ç–æ—Ç–∞ 1‚Äì60 s.

–°–ø–µ–∫—Ç—Ä—ã (–µ—Å–ª–∏ –µ—Å—Ç—å): NIR / Raman / FT-IR (–≤—ã—Å–æ–∫–∞—è —á–∞—Å—Ç–æ—Ç–∞ –∏–ª–∏ –ø–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∏–µ inline-—Å–∫–∞–Ω—ã).

MES / Batch metadata: batch_id, recipe_id, operator, lot —Å—ã—Ä—å—è.

LIMS / QC labels: HPLC/GC/NMR —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã (–¥–ª—è —Ä–µ—Ç—Ä–æ—Å–ø–µ–∫—Ç–∏–≤–Ω–æ–π —Å—É–ø–µ—Ä–≤–∞–π–∑-–≤–∞–ª–∏–¥–∞—Ü–∏–∏).

–õ–æ–≥–∏ –æ–±–æ—Ä—É–¥–æ–≤–∞–Ω–∏—è: vibration, temp bearings, current.

–ò—Å—Ç–æ—Ä–∏—è —Ä–µ–º–æ–Ω—Ç–æ–≤/–∏–Ω—Ü–∏–¥–µ–Ω—Ç–æ–≤ ‚Äî –¥–ª—è –º–µ—Ç—Ä–∏–∫ PdM.


–ï—Å–ª–∏ inline —Å–ø–µ–∫—Ç—Ä—ã –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç ‚Äî –≤—Å—ë —Ä–∞–≤–Ω–æ —Ä–∞–±–æ—Ç–∞–µ—Ç –º–Ω–æ–≥–æ—É—Ä–æ–≤–Ω–µ–≤—ã–π –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –ø–æ —Ñ–∏–∑–∏—á–µ—Å–∫–∏–º —Å–µ–Ω—Å–æ—Ä–∞–º.


---

3) Preprocessing ‚Äî –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –≤–∞–∂–Ω–æ

–°–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—è: –ø—Ä–∏–≤–µ—Å—Ç–∏ –≤—Å–µ —Å–∏–≥–Ω–∞–ª—ã –∫ –æ–±—â–µ–º—É —Ç–∞–π–º-–∏–Ω–¥–µ–∫—Å—É; –±–∏–Ω–Ω–∏–Ω–≥/—Ä–µ—Å–µ–º–ø–ª–∏–Ω–≥ (–Ω–∞–ø—Ä–∏–º–µ—Ä 5s/10s).

–î–µ—Ç—Ä–µ–Ω–¥–∏–Ω–≥: —É–±—Ä–∞—Ç—å —Å–µ–∑–æ–Ω–Ω–æ—Å—Ç—å/–¥–ª–∏–Ω–Ω—ã–µ —Ç—Ä–µ–Ω–¥—ã, —á—Ç–æ–±—ã –Ω–µ –≤–∑—Ä—ã–≤–∞—Ç—å false positives.

–§–∏–ª—å—Ç—Ä–∞—Ü–∏—è —à—É–º–∞: –º–µ–¥–∏–∞–Ω–Ω–∞—è/—Å–∫–æ–ª—å–∑—è—â–∞—è —Ñ–∏–ª—å—Ç—Ä–∞ –∏–ª–∏ low-pass –¥–ª—è –≤–∏–±—Ä–∞—Ü–∏–π/—à—É–º–Ω—ã—Ö –∫–∞–Ω–∞–ª–æ–≤.

–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è: per-sensor scaling (robust: median/IQR) ‚Äî –≤–∞–∂–Ω–∞ –¥–ª—è multivariate detection.

–ó–∞–ø–æ–ª–Ω–µ–Ω–∏–µ –ø—Ä–æ–ø—É—Å–∫–æ–≤: mark missing —Å —Ñ–ª–∞–≥–æ–º, –Ω–µ –ø—Ä–æ—Å—Ç–æ —Ñ—Ñ–∏–ª–ª ‚Äî –ø—Ä–æ–ø—É—Å–∫–∏ —Å–∞–º–∏ –º–æ–≥—É—Ç –±—ã—Ç—å —Å–∏–≥–Ω–∞–ª–æ–º.

Windowing: —Å–∫–æ–ª—å–∑—è—â–∏–µ –æ–∫–Ω–∞ (–Ω–∞–ø—Ä–∏–º–µ—Ä 30‚Äì300 s) –∏ –∞–≥—Ä–µ–≥–∞—Ü–∏–∏ (mean/std/slope/min/max/percentiles).

Feature drift monitoring: —Å–æ—Ö—Ä–∞–Ω—è—Ç—å baseline distribution –∏ –ø—Ä–æ–≤–µ—Ä—è—Ç—å drift (KS / population stability index).



---

4) Feature engineering ‚Äî —á—Ç–æ —Ä–µ–∞–ª—å–Ω–æ —Ä–∞–±–æ—Ç–∞–µ—Ç

Time-domain aggregates: mean/std/median/last/slope/area_under_curve per window.

Derivatives & rates: dT/dt, d(pH)/dt, relative change in flow.

Cross-features: difference (temp - coolant_temp), ratio (feedA/feedB).

Spectrum->features: PCA scores, select band integrals, wavelet coefficients.

Frequency features (–¥–ª—è –≤–∏–±—Ä–∞—Ü–∏–π): FFT peaks, spectral centroid, band energy.

Latent features: bottleneck output –∏–∑ –∞–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä–∞ / PCA ‚Äî —Å–∞–º–∏ —Å—Ç–∞–Ω–æ–≤—è—Ç—Å—è –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏.

Time-series embeddings: sliding-window flattened vectors, or sequence models (LSTM embedding).



---

5) –ê–ª–≥–æ—Ä–∏—Ç–º—ã ‚Äî –≤—ã–±–æ—Ä –∏ –æ–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ (—Å –ø–ª—é—Å–∞–º–∏/–º–∏–Ω—É—Å–∞–º–∏)

–ù–µ–Ω–∞–¥–∑–æ—Ä–Ω—ã–µ / –ø–æ–ª—É–Ω–∞–¥–∑–æ—Ä–Ω—ã–µ (–æ–±—ã—á–Ω–æ)

Isolation Forest ‚Äî –±—ã—Å—Ç—Ä—ã–π, –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º—ã–π, —Ö–æ—Ä–æ—à–æ –¥–ª—è —Ç–∞–±–ª–∏—á–Ω—ã—Ö –∞–≥—Ä–µ–≥–∞—Ç–æ–≤. (++ –Ω–∏–∑–∫–∞—è —Å–ª–æ–∂–Ω–æ—Å—Ç—å, ‚Äî —á—É–≤—Å—Ç–≤–∏—Ç–µ–ª–µ–Ω –∫ –Ω–∞—Å—Ç—Ä–æ–π–∫–µ contamination)

LOF / KNN-based ‚Äî –ª–æ–∫–∞–ª—å–Ω–∞—è –ø–ª–æ—Ç–Ω–æ—Å—Ç—å, —Ö–æ—Ä–æ—à–æ –Ω–∞ ¬´–ª–æ–∫–∞–ª—å–Ω—ã—Ö¬ª –∞–Ω–æ–º–∞–ª–∏—è—Ö.

One-Class SVM ‚Äî —Ä–∞–±–æ—Ç–∞–µ—Ç –ø—Ä–∏ –º–∞–ª—ã—Ö –¥–∞–Ω–Ω—ã—Ö –Ω–æ—Ä–º, –Ω–æ –º–µ–¥–ª–µ–Ω–Ω—ã–π.

Autoencoder (Dense/LSTM/Conv1D) ‚Äî reconstruction error per-feature; —Ö–æ—Ä–æ—à–æ –¥–ª—è multivariate temporal anomalies. (++ –≥–∏–±–∫–∏–π, –º–æ–∂–µ—Ç –≤—ã–¥–∞–≤–∞—Ç—å per-feature –æ—à–∏–±–∫–∏, ‚Äî —Ç—Ä–µ–±—É–µ—Ç —Ç—é–Ω–∏–Ω–≥–∞ + –¥–∞–Ω–Ω—ã–µ)

Variational Autoencoder / LSTM-AE ‚Äî —É—Å—Ç–æ–π—á–∏–≤–µ–µ –∫ —à—É–º—É, –Ω–æ —Å–ª–æ–∂–Ω–µ–µ.

Multivariate SPC / MPCA / Hotelling T2 ‚Äî –∫–ª–∞—Å—Å–∏–∫–∞ –¥–ª—è –ø—Ä–æ–º—ã—à–ª–µ–Ω–Ω–æ—Å—Ç–∏ (–ª–µ–≥–∫–æ –æ–±—ä—è—Å–Ω–∏—Ç—å –∞—É–¥–∏—Ç–æ—Ä–∏–∏).

Change-point detection (ruptures, Bayesian) ‚Äî –ª–æ–≤–∏—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω—ã–µ —Å–¥–≤–∏–≥–∏ (drift, step-change).

Ensemble / voting: —Å–æ—á–µ—Ç–∞–Ω–∏–µ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –º–µ—Ç–æ–¥–æ–≤ —Å–Ω–∏–∂–∞–µ—Ç FP.


–°—Ç—Ä–∏–º–∏–Ω–≥/–æ–Ω–ª–∞–π–Ω

River (—Ä–∞–Ω–µ–µ creme) –¥–ª—è –æ–Ω–ª–∞–π–Ω-–æ–±—É—á–µ–Ω–∏—è / –∞–¥–∞–ø—Ç–∞—Ü–∏–∏.

EWMA / CUSUM ‚Äî –±—ã—Å—Ç—Ä—ã–µ –ø—Ä–æ—Å—Ç—ã–µ –¥–µ—Ç–µ–∫—Ç–æ—Ä—ã –Ω–∞ –æ–¥–Ω–æ–º —Å–∏–≥–Ω–∞–ª–µ.


–ö–∞–∫ –≤—ã–±–∏—Ä–∞—Ç—å

–ï—Å–ª–∏ –µ—Å—Ç—å –ª–µ–π–±–ª—ã (historic OOS/–æ—Ç–∫–∞–∑—ã) ‚Äî –º–æ–∂–Ω–æ –ø–µ—Ä–µ–π—Ç–∏ –∫ supervised (XGBoost –Ω–∞ –ø—Ä–µ–¥–∏–∫—Ç–∞—Ö/—Ñ–∏—á–∞—Ö), –ª–∏–±–æ train isolation with labels for thresholding.

–ë–µ–∑ –ª–µ–π–±–ª–æ–≤ ‚Äî –Ω–∞—á–Ω–∏—Ç–µ —Å IsolationForest + MPCA + –ø—Ä–æ—Å—Ç—ã—Ö change-point. –î–æ–±–∞–≤–ª—è–π—Ç–µ AE, –µ—Å–ª–∏ —Ö–æ—Ç–∏—Ç–µ –ø–æ–π–º–∞—Ç—å —Å–ª–æ–∂–Ω—ã–µ multivariate –ø–∞—Ç—Ç–µ—Ä–Ω—ã.



---

6) –í–∞–ª–∏–¥–∞—Ü–∏—è –∏ –º–µ—Ç—Ä–∏–∫–∏ (—Ä–µ–∞–ª—å–Ω–æ –≤–∞–∂–Ω—ã–µ)

–ê–Ω–æ–º–∞–ª–∏—è ‚Äî class imbalance. –ù—É–∂–Ω—ã —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏:

Precision@k: –≤–∞–∂–µ–Ω, —Ç.–∫. –æ–ø–µ—Ä–∞—Ç–æ—Ä—ã –Ω–µ –¥–æ–ª–∂–Ω—ã —Ç–æ–Ω—É—Ç—å –≤ –ª–æ–∂–Ω—ã—Ö —Ç—Ä–µ–≤–æ–≥–∞—Ö.

Recall / Recall@lead_time: –Ω–∞—Å–∫–æ–ª—å–∫–æ –±—ã—Å—Ç—Ä–æ –º–æ–¥–µ–ª—å –∑–∞–º–µ—á–∞–µ—Ç –ø—Ä–µ–¥–≤–µ—Å—Ç–Ω–∏–∫–∏ OOS.

F1 (balanced view).

PR-AUC –≤–º–µ—Å—Ç–æ ROC-AUC –ø—Ä–∏ —Å–∏–ª—å–Ω–æ–º –¥–∏—Å–±–∞–ª–∞–Ω—Å–µ.

False Alarm Rate (FAR) per day/shift ‚Äî –±–∏–∑–Ω–µ—Å-–ø–æ–∫–∞–∑–∞—Ç–µ–ª—å.

Mean Time To Detect (MTTD) / median lead time (–≤ –º–∏–Ω—É—Ç–∞—Ö/—á–∞—Å–∞—Ö –¥–æ OOS).

Detection Delay distribution (—Å–º–µ—â–µ–Ω–∏–µ —Ä–∞–Ω–Ω–∏—Ö —Å—Ä–∞–±–∞—Ç—ã–≤–∞–Ω–∏–π).

Stability metrics: Frequency of alerts per unit –≤—Ä–µ–º–µ–Ω–∏ (—É–¥–µ—Ä–∂–∏–≤–∞—Ç—å –≤ –ø—Ä–∏–µ–º–ª–µ–º—ã—Ö –ø—Ä–µ–¥–µ–ª–∞—Ö).


–í–∞–ª–∏–¥–∞—Ü–∏—è-–ø–æ–¥—Ö–æ–¥—ã:

–ï—Å–ª–∏ –ª–µ–π–±–ª—ã –µ—Å—Ç—å ‚Äî —Å–¥–µ–ª–∞—Ç—å time-aware split (train on older batches).

–ï—Å–ª–∏ –ª–µ–π–±–ª–æ–≤ –Ω–µ—Ç ‚Äî inject synthetic anomalies (–∫–æ–Ω—Ç—Ä–∞—Å—Ç–Ω—ã–µ —Å—Ü–µ–Ω–∞—Ä–∏–∏), run backtest on historical normal data, measure FP rate.

Shadow mode: 2‚Äì4 –Ω–µ–¥–µ–ª–∏ ‚Äî —Å–∏—Å—Ç–µ–º–∞ –ø–∏—à–µ—Ç –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è, –æ–ø–µ—Ä–∞—Ç–æ—Ä—ã –Ω–µ –≤–º–µ—à–∏–≤–∞—é—Ç—Å—è; –ø–æ—Ç–æ–º —Å—Ä–∞–≤–Ω–∏–≤–∞—Ç—å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ QC.



---

7) Thresholding –∏ —Å–∏–≥–Ω–∞–ª-—Ç—Ä–µ—É–≥–æ–ª—å–Ω–∏–∫

–î–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–π –ø–æ—Ä–æ–≥: percentile-based (–Ω–∞–ø—Ä–∏–º–µ—Ä, score > 99.5-percentile of last 24h) –∏–ª–∏ EWMA adapt.

–¢—Ä–∏ —É—Ä–æ–≤–Ω—è —Ç—Ä–µ–≤–æ–≥: INFO / WARNING / CRITICAL, —Å —Ä–∞–∑–Ω—ã–º–∏ SOP: INFO ‚Äî –º–æ–Ω–∏—Ç–æ—Ä–∏—Ç—å, WARNING ‚Äî –≤–∑—è—Ç—å –ø—Ä–æ–±—É, CRITICAL ‚Äî –æ—Å—Ç–∞–Ω–æ–≤–∏—Ç—å/–∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∞—Ç—å.

Consensus rule: —Ç—Ä–µ–±–æ–≤–∞—Ç—å —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è 2/3 –º–æ–¥–µ–ª–µ–π (IsolationForest + AE + MPCA) –¥–ª—è CRITICAL ‚Äî —É–º–µ–Ω—å—à–∞–µ—Ç FP.

Human-in-loop: –∫–∞–∂–¥—ã–π –∞–ª–µ—Ä—Ç –ª–æ–≥–∏—Ä—É–µ—Ç—Å—è, –æ–ø–µ—Ä–∞—Ç–æ—Ä –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–∞–µ—Ç/–æ—Ç–∫–ª–æ–Ω—è–µ—Ç ‚Äî —ç—Ç–æ –º–µ—Ç–∫–∏ –¥–ª—è –±—É–¥—É—â–µ–π supervised-–æ–±—É—á–∞–ª–∫–∏.



---

8) Explainability / Root cause

Reconstruction error per sensor (–¥–ª—è AE) ‚Äî —Å–∞–º—ã–π –ø—Ä—è–º–æ–π –∫–∞–Ω–∞–ª: ¬´–≤–∫–ª–∞–¥¬ª –ø–æ –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π.

Feature-contribution via SHAP ‚Äî –ø—Ä–∏–º–µ–Ω–∏–º–æ –µ—Å–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç–µ supervised detector (–∏–ª–∏ pseudo-labels).

Correlation-break detection: –¥–æ/–ø–æ—Å–ª–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ –∫–æ—Ä—Ä. –º–∞—Ç—Ä–∏—Ü ‚Äî –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, –∫–∞–∫–∏–µ —Å–≤—è–∑–∏ —Ä–∞–∑–æ—Ä–≤–∞–ª–∏—Å—å.

Nearest-anomalies search: –Ω–∞–π—Ç–∏ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–µ –ø–æ—Ö–æ–∂–∏–µ —Å–ª—É—á–∞–∏ —Å –∏–∑–≤–µ—Å—Ç–Ω–æ–π –ø—Ä–∏—á–∏–Ω–æ–π.

–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –ø–∞–∫–µ—Ç –¥–ª—è –æ–ø–µ—Ä–∞—Ç–æ—Ä–∞: top-3 sensors, time window, suggested action.


–û–±—ä—è—Å–Ω–∏–º–æ—Å—Ç—å ‚Äî –∫–ª—é—á –¥–ª—è –æ–¥–æ–±—Ä–µ–Ω–∏—è —Ä–µ–≥—É–ª—è—Ç–æ—Ä–∞ –∏ –¥–æ–≤–µ—Ä–∏—è –æ–ø–µ—Ä–∞—Ç–æ—Ä–æ–≤.


---

9) Deployment (–ø—Ä–∞–∫—Ç–∏—á–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞)

Ingest SCADA ‚Üí Kafka / MQTT.

Preprocessing (stream processor): Spark Streaming / Flink / lightweight service.

Scoring: –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä FastAPI (models in Torch/Sklearn) ‚Üí produce anomaly score to Kafka topic.

Alerting: rule engine (Grafana alerts / custom) ‚Üí send to Slack/SMS/SCADA alarm.

Storage: TSDB (Influx/Grafana) –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–π + archival in S3 for offline retraining.

Model ops: MLflow for model registry; scheduled retrain (weekly) with data drift checks.

Monitoring: Prometheus (latency), Grafana (scores, number of alerts), data-quality (Great Expectations).



---

10) –†–∏—Å–∫–∏ –∏ –º–∏—Ç–∏–≥–µ–π—Ç—ã

False positives ‚Üí ensemble + dynamic thresholds + operator feedback loop.

Data drift ‚Üí drift detection triggers retrain + retraining pipeline.

Sensor failures mistaken for anomalies ‚Üí redundancy checks, sensor health channel.

Operator distrust ‚Üí shadow mode + –æ–±—É—á–µ–Ω–∏–µ + explainability tiles.

Regulatory trace ‚Üí audit trails (why alert, model version, input values).



---

11) ROI & –±–∏–∑–Ω–µ—Å-—ç—Ñ—Ñ–µ–∫—Ç (–∫—Ä–∞—Ç–∫–æ, —Ü–∏—Ñ—Ä—ã –¥–ª—è —Å–ª–∞–π–¥–∞)

–£–º–µ–Ω—å—à–µ–Ω–∏–µ OOS/–±—Ä–∞–∫–∞ –Ω–∞ 30% (–∫–æ–Ω—Ç—É—Ä A+B –≤ —Å–≤—è–∑–∫–µ) ‚Äî —ç–∫–æ–Ω–æ–º–∏—è –¥–µ—Å—è—Ç–∫–∏ % –æ—Ç —Ç–µ–∫—É—â–∏—Ö –ø–æ—Ç–µ—Ä—å.

–ü—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏–µ –∞–≤–∞—Ä–∏–π/–Ω–µ–∑–∞–ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –ø—Ä–æ—Å—Ç–æ–µ–≤ (PdM –∏–Ω—Ç–µ–≥—Ä.) ‚Äî —ç–∫–æ–Ω–æ–º–∏—è $3‚Äì5M/–≥–æ–¥ (–æ—Ü–µ–Ω–æ—á–Ω–æ, –∑–∞–≤–∏—Å–∏—Ç –æ—Ç –º–∞—Å—à—Ç–∞–±–∞).

–ë—ã—Å—Ç—Ä–∞—è –ª–æ–∫–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ–±–ª–µ–º—ã (root-cause) ‚Äî —Å–æ–∫—Ä–∞—â–µ–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–∏ —Ä–µ–∞–≥–∏—Ä–æ–≤–∞–Ω–∏—è –Ω–∞ 50% ‚Üí —É–º–µ–Ω—å—à–µ–Ω–∏–µ –ø–µ—Ä–µ—Ä–∞–±–æ—Ç–æ–∫.
(–î–ª—è —Å–ª–∞–π–¥–∞: —É–∫–∞–∂–∏—Ç–µ —Ä–µ–∞–ª—å–Ω—ã–µ —á–∏—Å–ª–∞ –∫–æ–º–ø–∞–Ω–∏–∏ + –ø—Ä–∏–≤–µ–¥–∏—Ç–µ conservative estimate; –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ —Ä–∞—Å—á—ë—Ç, –∫–∞–∫ –≤ Contour A ROI-—Å–ª–∞–π–¥–µ.)



---

12) MVP –¥–ª—è —Ö–∞–∫–∞—Ç–æ–Ω–∞ (—á—Ç–æ –Ω—É–∂–Ω–æ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–∏—Ç—å –∂—é—Ä–∏)

1. –°–∫—Ä–∏–ø—Ç / –Ω–æ—É—Ç–±—É–∫: –∑–∞–≥—Ä—É–∂–∞–µ—Ç data_B.csv (–µ—Å–ª–∏ –µ—Å—Ç—å) –∏–ª–∏ –≥–µ–Ω–µ—Ä–∏—Ç —Å–∏–Ω—Ç–µ—Ç–∏–∫—É.


2. –ú–æ–¥–µ–ª—å IsolationForest (–æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ) + –æ–ø—Ü–∏—è Autoencoder.


3. –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏:

timeline: sensor(s) + anomaly flag (—Ü–≤–µ—Ç–æ–º)

histogram anomaly scores + chosen threshold

table: –Ω–∞–π–¥–µ–Ω–Ω—ã–µ incidents (start, end, top-3 contributing sensors)

metrics: precision/recall (–µ—Å–ª–∏ –µ—Å—Ç—å labels) / expected lead time



4. README: –∫–∞–∫ –∑–∞–ø—É—Å—Ç–∏—Ç—å, –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ —á—Ç–æ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç.


5. –ö–æ–¥ –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º —Ñ–∞–π–ª–µ contour_b.py + PNG —Å–∫—Ä–∏–Ω–æ–≤ –¥–ª—è –ø—Ä–µ–∑–µ–Ω—Ç–∞—Ü–∏–∏.




---

13) –ö–∞–∫ —ç—Ç–æ –ø–æ–≤—ã—à–∞–µ—Ç –æ—Ü–µ–Ω–∫—É –ø–æ –∫—Ä–∏—Ç–µ—Ä–∏—è–º –∂—é—Ä–∏

–ê–∫—Ç—É–∞–ª—å–Ω–æ—Å—Ç—å: –≤—ã –ª–æ–≤–∏—Ç–µ —Ä–µ–∞–ª—å–Ω—ã–µ –ø—Ä–µ–¥–≤–µ—Å—Ç–Ω–∏–∫–∏ –±—Ä–∞–∫–∞ ‚Äî –ø—Ä—è–º–æ–µ —É–º–µ–Ω—å—à–µ–Ω–∏–µ OOS.

–ò–Ω–Ω–æ–≤–∞—Ü–∏–∏: —Å–æ—á–µ—Ç–∞–Ω–∏–µ multivariate AE + MPCA + streaming detection + explainability.

–ë–∏–∑–Ω–µ—Å: –±—ã—Å—Ç—Ä–æ–¥–µ–π—Å—Ç–≤–∏–µ ‚Üí –±—ã—Å—Ç—Ä—ã–π ROI, –º–µ–Ω—å—à–µ –æ—Ç—Ö–æ–¥–æ–≤, –º–µ–Ω—å—à–µ –ø—Ä–æ—Å—Ç–æ–µ–≤.

–¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–π –≤–∫–ª–∞–¥: –ø–æ–ª–Ω–æ—Ü–µ–Ω–Ω—ã–π pipeline, –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ–º—ã–π –≤ notebook/—Ä–µ–ø–æ.

–ü—Ä–µ–∑–µ–Ω—Ç–∞—Ü–∏—è: —á–µ—Ç–∫–∏–µ –≥—Ä–∞—Ñ–∏–∫–∏/–º–µ—Ç—Ä–∏–∫–∏ ‚Äî –ª–µ–≥–∫–æ –ø–æ–Ω—è—Ç—å.



---




---

15) –ì–æ—Ç–æ–≤—ã–π —Ä–∞–±–æ—á–∏–π –∫–æ–¥ (–≤—Å—Ç–∞–≤—å –≤ contour_b.py)

–≠—Ç–æ—Ç —Å–∫—Ä–∏–ø—Ç: –ø—ã—Ç–∞–µ—Ç—Å—è –∑–∞–≥—Ä—É–∑–∏—Ç—å data_B.csv, –∏–Ω–∞—á–µ –≥–µ–Ω–µ—Ä–∏—Ç —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—É—é —Å–∏–Ω—Ç–µ—Ç–∏–∫—É, –æ–±—É—á–∞–µ—Ç IsolationForest, —Ä–∏—Å—É–µ—Ç –≥—Ä–∞—Ñ–∏–∫–∏ –∏ (–µ—Å–ª–∏ –º–µ—Ç–∫–∏ –µ—Å—Ç—å) —Å—á–∏—Ç–∞–µ—Ç precision/recall. –ú–æ–∂–Ω–æ –∑–∞–ª–∏—Ç—å –≤ GitHub –∫–∞–∫ MVP.

# contour_b.py
# Python 3.8+
# pip install numpy pandas matplotlib scikit-learn seaborn

import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.ensemble import IsolationForest
from sklearn.metrics import precision_score, recall_score, f1_score

sns.set(style="whitegrid")

# --- 1. Load or generate data ---
if os.path.exists("data_B.csv"):
    df = pd.read_csv("data_B.csv", parse_dates=["timestamp"])
    # Expect columns: timestamp,temp,pH,flow,torque,vibration,label (optional 0/1)
else:
    np.random.seed(42)
    T = 2000
    time = pd.date_range("2025-01-01", periods=T, freq="T")  # 1-minute steps
    temp = 72 + np.random.normal(0, 0.3, T).cumsum()*0.001 + np.sin(np.linspace(0, 20, T))*0.2
    pH = 7.2 + np.random.normal(0, 0.02, T)
    flow = 1.0 + np.random.normal(0, 0.02, T)
    torque = 40 + np.random.normal(0, 0.5, T)
    vibration = np.random.normal(0, 0.2, T)

    # inject anomalies: batches with drift + spikes
    labels = np.zeros(T, dtype=int)
    for start in [600, 1200, 1600]:
        idx = np.arange(start, start+30)
        temp[idx] += np.linspace(0.5, 2.0, len(idx))  # drift up
        pH[idx] += np.random.normal(0.2, 0.05, len(idx))
        flow[idx] *= 0.8
        torque[idx] += np.random.normal(3.0, 0.5, len(idx))
        vibration[idx] += np.random.normal(1.5, 0.4, len(idx))
        labels[idx] = 1

    df = pd.DataFrame({
        "timestamp": time,
        "temp": temp,
        "pH": pH,
        "flow": flow,
        "torque": torque,
        "vibration": vibration,
        "label": labels
    })

# --- 2. Feature engineering: sliding window aggregates (window=5) ---
df = df.sort_values("timestamp").reset_index(drop=True)
window = 5
agg = pd.DataFrame()
for col in ["temp", "pH", "flow", "torque", "vibration"]:
    agg[f"{col}_mean"] = df[col].rolling(window, min_periods=1).mean()
    agg[f"{col}_std"]  = df[col].rolling(window, min_periods=1).std().fillna(0)
    agg[f"{col}_last"] = df[col].shift(0)
    agg[f"{col}_slope"] = df[col].diff().rolling(window, min_periods=1).mean().fillna(0)

agg = agg.fillna(method="bfill").fillna(0)

# --- 3. Train IsolationForest on first 70% (simulate time-aware) ---
split_idx = int(0.7 * len(agg))
X_train = agg.iloc[:split_idx].values
X_test  = agg.iloc[split_idx:].values

clf = IsolationForest(n_estimators=200, contamination=0.02, random_state=42)
clf.fit(X_train)

scores = -clf.decision_function(agg.values)  # higher = more anomalous
threshold = np.percentile(scores[:split_idx], 98)  # dynamic threshold based on train

anomaly_flag = (scores > threshold).astype(int)

df["anomaly_score"] = scores
df["anomaly_flag"] = anomaly_flag

# --- 4. Metrics (if label exists) ---
if "label" in df.columns:
    # align labels (test period only) - evaluate on the test segment
    true = df["label"].values[split_idx:]
    pred = df["anomaly_flag"].values[split_idx:]
    prec = precision_score(true, pred, zero_division=0)
    rec = recall_score(true, pred, zero_division=0)
    f1 = f1_score(true, pred, zero_division=0)
    print(f"Evaluation on test period: Precision={prec:.3f} Recall={rec:.3f} F1={f1:.3f}")

# --- 5. Visualizations ---
plt.figure(figsize=(14,6))
plt.plot(df["timestamp"], df["temp"], label="temp")
plt.scatter(df["timestamp"][df["anomaly_flag"]==1], df["temp"][df["anomaly_flag"]==1],
            color="red", s=20, label="anomaly")
plt.title("Temp timeline with anomalies")
plt.legend()
plt.tight_layout()
plt.savefig("contourB_temp_timeline.png")
plt.show()

plt.figure(figsize=(14,3))
plt.plot(df["timestamp"], df["anomaly_score"], label="anomaly_score")
plt.axhline(threshold, color="red", linestyle="--", label=f"threshold={threshold:.3f}")
plt.title("Anomaly score timeline")
plt.legend()
plt.tight_layout()
plt.savefig("contourB_score_timeline.png")
plt.show()

plt.figure(figsize=(8,4))
sns.histplot(df["anomaly_score"], bins=80, kde=True)
plt.axvline(threshold, color="red", linestyle="--")
plt.title("Anomaly score distribution")
plt.tight_layout()
plt.savefig("contourB_score_hist.png")
plt.show()

# --- 6. Incident table (merge contiguous anomaly windows) ---
df["anomaly_group"] = (df["anomaly_flag"].diff(1) != 0).cumsum() * df["anomaly_flag"]
incidents = []
for gid, g in df.groupby("anomaly_group"):
    if gid == 0: continue
    start = g["timestamp"].iloc[0]
    end = g["timestamp"].iloc[-1]
    top_features = agg.iloc[g.index].mean().sort_values(ascending=False).head(3).index.tolist()
    incidents.append({"group":int(gid), "start":start, "end":end, "top_features":top_features})
inc_df = pd.DataFrame(incidents)
print("\nDetected incidents:")
print(inc_df)

# Save results
df.to_csv("contourB_results.csv", index=False)
inc_df.to_csv("contourB_incidents.csv", index=False)
print("\nSaved results: contourB_results.csv, contourB_incidents.csv")

–ß—Ç–æ –¥–µ–ª–∞–µ—Ç —ç—Ç–æ—Ç —Å–∫—Ä–∏–ø—Ç:

–ì–µ–Ω–µ—Ä–∏—Ç —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—É—é –≤—Ä–µ–º–µ–Ω–Ω—É—é —Å–µ—Ä–∏—é —Å 5 –∫–∞–Ω–∞–ª–∞–º–∏ –∏ —Ç—Ä–µ–º—è –≤—Å—Ç—Ä–æ–µ–Ω–Ω—ã–º–∏ –∞–Ω–æ–º–∞–ª–∏—è–º–∏.

–î–µ–ª–∞–µ—Ç –ø—Ä–æ—Å—Ç—ã–µ window-–∞–≥—Ä–µ–≥–∞—Ü–∏–∏.

–û–±—É—á–∞–µ—Ç IsolationForest –Ω–∞ ¬´–Ω–æ—Ä–º–∞–ª—å–Ω–æ–π¬ª —á–∞—Å—Ç–∏ (time-aware).

–í—ã–¥–∞—ë—Ç anomaly score, –±–∏–Ω–∞—Ä–Ω—ã–π —Ñ–ª–∞–≥ –ø–æ –ø–æ—Ä–æ–≥—É, —Ä–∏—Å—É–µ—Ç timeline –∏ histogram.

–≠–∫—Å–ø–æ—Ä—Ç–∏—Ä—É–µ—Ç contourB_results.csv –∏ contourB_incidents.csv ‚Äî —ç—Ç–∏ —Ñ–∞–π–ª—ã –º–æ–∂–Ω–æ –∑–∞–≥—Ä—É–∑–∏—Ç—å –≤ –ø—Ä–µ–∑–µ–Ω—Ç–∞—Ü–∏—é/README.



---


---

17) –ö–∞–∫ —ç—Ç–æ —Å–≤—è–∑–∞—Ç—å —Å –¥—Ä—É–≥–∏–º–∏ –∫–æ–Ω—Ç—É—Ä–∞–º–∏ (—Å–∏–Ω–µ—Ä–≥–∏—è)

Contour A (soft-sensors) –¥–∞—ë—Ç –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ purity; Contour B –¥–∞—ë—Ç —Ä–∞–Ω–Ω–∏–µ –∞–Ω–æ–º–∞–ª–∏–∏ –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ. B ‚Üí A: —Ä–∞–Ω–Ω—è—è —Ç—Ä–µ–≤–æ–≥–∞ –º–æ–∂–µ—Ç –≤–∫–ª—é—á–∞—Ç—å –ø–µ—Ä–µ—Å—á—ë—Ç A (soft-sensor) –∏ –ø–µ—Ä–µ—Å—ã–ª–∫—É —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏.

Contour C (PdM / Optimization): –∞–Ω–æ–º–∞–ª–∏–∏ –æ–±–æ—Ä—É–¥–æ–≤–∞–Ω–∏—è –∏–∑ B –º–æ–≥—É—Ç —Ç—Ä–∏–≥–≥–µ—Ä–∏—Ç—å PdM. –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä C –∏—Å–ø–æ–ª—å–∑—É–µ—Ç B –∫–∞–∫ —Å–∏–≥–Ω–∞–ª –ø–ª–æ—Ö–æ–≥–æ —Ä–µ–∂–∏–º–∞ –¥–ª—è DoE / BO.
–í–º–µ—Å—Ç–µ –¥–∞—ë—Ç: —Ä–∞–Ω–Ω–µ–µ –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ (B) ‚Üí –±—ã—Å—Ç—Ä—ã–µ –∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∫–∏ (A) ‚Üí –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ –ø—Ä–æ—Ü–µ—Å—Å–∞ –∏ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç–µ—Ö.–æ–±—Å–ª—É–∂–∏–≤–∞–Ω–∏—è (C).



---

18) –°–ª–µ–¥—É—é—â–∏–µ —à–∞–≥–∏ (–±—ã—Å—Ç—Ä–æ –∏ –ø—Ä–∞–≥–º–∞—Ç–∏—á–Ω–æ)

1. –ó–∞–ø—É—Å—Ç–∏—Ç—å contour_b.py –ª–æ–∫–∞–ª—å–Ω–æ ‚Äî —Å–æ–±—Ä–∞—Ç—å PNG –∏ CSV.


2. –ü–æ–ª–æ–∂–∏—Ç—å —Å–∫—Ä–∏–ø—Ç + PNG + README –≤ GitHub (–æ—Ç–¥–µ–ª—å–Ω–∞—è –ø–∞–ø–∫–∞ contours/contour_b/).


3. –ü–æ–ª—É—á–∏—Ç—å –∏ –∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Ç–µ–∫—Å—Ç: timeline image, hist image, table incidents (2‚Äì3 —Å—Ç—Ä–æ—á–∫–∏), –∫—Ä–∞—Ç–∫—É—é –º–µ—Ç—Ä–∏–∫—É (Precision, Recall, –µ—Å–ª–∏ –µ—Å—Ç—å –ª–µ–π–±–ª) –∏ –±–∏–∑–Ω–µ—Å-—ç—Ñ—Ñ–µ–∫—Ç (lead time ‚Üí $ saved estimate).


4. –ü–µ—Ä–µ–∫—Ä—ë—Å—Ç–Ω–æ —Å—Å—ã–ª–∞—Ç—å –Ω–∞ Contour A (show how B reduces false positives & increases lead time).




















 –ö–æ–Ω—Ç—É—Ä C (Predictive Maintenance / PdM) —Ç–∞–∫ –∂–µ –ø–æ–¥—Ä–æ–±–Ω–æ –∏ –ø—Ä–∞–∫—Ç–∏—á–Ω–æ–∏: —á—Ç–æ –æ–Ω –¥–∞—ë—Ç, –∫–∞–∫–∏–µ –¥–∞–Ω–Ω—ã–µ –∏ —Ñ–∏—á–∏ –Ω—É–∂–Ω—ã, –∫–∞–∫–∏–µ –º–æ–¥–µ–ª–∏ –∏ –º–µ—Ç—Ä–∏–∫–∏, deployment, ROI, —Ä–∏—Å–∫–∏ ‚Äî –∏ –≥–æ—Ç–æ–≤—ã–π —Ä–∞–±–æ—á–∏–π —Å–∫—Ä–∏–ø—Ç contour_c.py, –∫–æ—Ç–æ—Ä—ã–π –º–æ–∂–Ω–æ –ø—Ä—è–º–æ –ø–æ–ª–æ–∂–∏—Ç—å –≤ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π (–µ—Å–ª–∏ –Ω–µ—Ç —Ä–µ–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö ‚Äî –æ–Ω —Å–∞–º —Å–≥–µ–Ω–µ—Ä–∏—Ç –ø—Ä–∞–≤–¥–æ–ø–æ–¥–æ–±–Ω—É—é —Å–∏–Ω—Ç–µ—Ç–∏–∫—É –∏ –≤—ã–≤–µ–¥–µ—Ç –≥—Ä–∞—Ñ–∏–∫–∏/CSV –¥–ª—è –ø—Ä–µ–∑–µ–Ω—Ç–∞—Ü–∏–∏).
–ù–∏–∂–µ ‚Äî –∫–æ–º–ø–∞–∫—Ç–Ω—ã–π, –Ω–æ –ø–æ–ª–Ω—ã–π –±–ª–æ–∫, –∫–æ—Ç–æ—Ä—ã–π –≤—ã –º–æ–∂–µ—Ç–µ –≤—Å—Ç–∞–≤–∏—Ç—å –≤ README.md –∏–ª–∏ –≤ —Å–ª–∞–π–¥ ¬´–ö–æ–Ω—Ç—É—Ä C¬ª.

–ö–æ–Ω—Ç—É—Ä C ‚Äî Predictive Maintenance (PdM) ‚Äî –ø–æ–ª–Ω—ã–π –ø—Ä–∞–∫—Ç–∏—á–Ω—ã–π —Ä–∞–∑–±–æ—Ä

–ö–æ—Ä–æ—Ç–∫–æ: —Ü–µ–ª—å ‚Äî –ø—Ä–µ–¥—Å–∫–∞–∑–∞—Ç—å –ø–æ–ª–æ–º–∫—É/—Å–Ω–∏–∂–µ–Ω–∏–µ —Ä–µ—Å—É—Ä—Å–∞ –æ–±–æ—Ä—É–¥–æ–≤–∞–Ω–∏—è (RUL / –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –æ—Ç–∫–∞–∑–∞ –≤ –∑–∞–¥–∞–Ω–Ω–æ–º –≥–æ—Ä–∏–∑–æ–Ω—Ç–µ) –∑–∞—Ä–∞–Ω–µ–µ, —á—Ç–æ–±—ã –ø–ª–∞–Ω–∏—Ä–æ–≤–∞—Ç—å —Ç–µ—Ö–æ–±—Å–ª—É–∂–∏–≤–∞–Ω–∏–µ –¥–æ –∞–≤–∞—Ä–∏–∏ –∏ —Å–æ–∫—Ä–∞—Ç–∏—Ç—å –Ω–µ–∑–∞–ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø—Ä–æ—Å—Ç–æ–∏.


---

1) –ß—Ç–æ –≤—ã–¥–∞—ë—Ç –∫–æ–Ω—Ç—É—Ä C ‚Äî –∞—Ä—Ç–µ—Ñ–∞–∫—Ç—ã

RUL (remaining useful life) ‚Äî –ø—Ä–æ–≥–Ω–æ–∑ –æ—Å—Ç–∞–≤—à–µ–≥–æ—Å—è –≤—Ä–µ–º–µ–Ω–∏/—Ü–∏–∫–ª–æ–≤ –¥–æ –æ—Ç–∫–∞–∑–∞ (—Ä–µ–≥—Ä–µ—Å—Å–∏—è).

Probability of failure –≤ –≥–æ—Ä–∏–∑–æ–Ω—Ç–µ H (–Ω–∞–ø—Ä. 24‚Äì72 —á–∞—Å–∞) ‚Äî –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è (–≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å / —Ä–∏—Å–∫).

Alert / maintenance ticket ‚Äî –º–∞—à–∏–Ω–∞ X: P(failure)<0.2 ‚Äî –º–æ–Ω–∏—Ç–æ—Ä–∏—Ç—å, >0.6 ‚Äî –ø–ª–∞–Ω–æ–≤–æ–µ –¢–û, >0.85 ‚Äî —Å—Ä–æ—á–Ω–æ–µ –≤–º–µ—à–∞—Ç–µ–ª—å—Å—Ç–≤–æ.

–î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞/—Ñ–∏—á–∏ —Å –Ω–∞–∏–±–æ–ª—å—à–∏–º –≤–∫–ª–∞–¥–æ–º ‚Äî –∫–∞–∫–∏–µ –ø–æ–∫–∞–∑–∞—Ç–µ–ª–∏ (–≤–∏–±—Ä–∞—Ü–∏—è, —Ç–æ–∫, —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞) —Ä–∞—Å—Ç—É—Ç –ø–µ—Ä–µ–¥ –æ—Ç–∫–∞–∑–æ–º.

–í—Ä–µ–º–µ–Ω–Ω–∞—è –ª–∏–Ω–∏—è –¥–ª—è –∫–∞–∂–¥–æ–π –º–∞—à–∏–Ω—ã: –º–µ—Ç—Ä–∏–∫–∏ + –æ–∫–Ω–æ –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è (lead time).

–§–∞–π–ª—ã: contourC_results.csv, contourC_metrics.txt, PNG-–≥—Ä–∞—Ñ–∏–∫–∏ (RUL scatter, ROC, feature-importance, timeline).



---

2) –ö–∞–∫–∏–µ –¥–∞–Ω–Ω—ã–µ –Ω—É–∂–Ω—ã

–û–Ω–ª–∞–π–Ω (SCADA/DCS): –≤–∏–±—Ä–∞—Ü–∏—è, —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –ø–æ–¥—à–∏–ø–Ω–∏–∫–æ–≤, —Ç–æ–∫/–º–æ—â–Ω–æ—Å—Ç—å –¥–≤–∏–≥–∞—Ç–µ–ª—è, –¥–∞–≤–ª–µ–Ω–∏–µ, —Å–∫–æ—Ä–æ—Å—Ç—å/–∫—Ä—É—Ç—è—â–∏–π –º–æ–º–µ–Ω—Ç ‚Äî —á–∞—Å—Ç–æ—Ç–∞, –Ω–∞–ø—Ä–∏–º–µ—Ä 1‚Äì60s.

–õ–æ–≥–∏ CMMS: –∏—Å—Ç–æ—Ä–∏—è —Ä–µ–º–æ–Ω—Ç–æ–≤, —Ç–∏–ø –ø–æ–ª–æ–º–∫–∏, –≤—Ä–µ–º—è –ø—Ä–æ—Å—Ç–æ—è.

MES: machine_id, shift, –æ–ø–µ—Ä–∞—Ç–æ—Ä, batch_id (–µ—Å–ª–∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ).

–î–∞—Ç–∞-—à—Ç–∞–º–ø–∞–º–∏: timestamps —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∏—Ä–æ–≤–∞–Ω—ã.

–ú–µ—Ç–∫–∏ (–µ—Å–ª–∏ –µ—Å—Ç—å): –≤—Ä–µ–º—è –æ—Ç–∫–∞–∑–∞, —Ç–∏–ø –æ—Ç–∫–∞–∑–∞ (–¥–ª—è supervised/ survival). –ë–µ–∑ –º–µ—Ç–æ–∫ ‚Äî –º–æ–∂–Ω–æ –¥–µ–ª–∞—Ç—å anomaly-based PdM / unsupervised detection.



---

3) –ö–∞–∫ —Ñ–æ—Ä–º—É–ª–∏—Ä—É–µ–º –∑–∞–¥–∞—á—É (—Ü–µ–ª–∏/—Ç–∞—Ä–≥–µ—Ç—ã)

RUL (regression): –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞—Ç—å –æ—Å—Ç–∞–≤—à–µ–µ—Å—è –≤—Ä–µ–º—è/—Ü–∏–∫–ª—ã –¥–æ –æ—Ç–∫–∞–∑–∞.

Failure-in-horizon (classification): –ø—Ä–µ–¥—Å–∫–∞–∑–∞—Ç—å, —Å–ª—É—á–∏—Ç—Å—è –ª–∏ –æ—Ç–∫–∞–∑ –≤ –±–ª–∏–∂–∞–π—à–∏–µ H –µ–¥–∏–Ω–∏—Ü –≤—Ä–µ–º–µ–Ω–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä, H=48 —á–∞—Å–æ–≤/50 —Ü–∏–∫–ª–æ–≤).

–ê–≥—Ä–µ–≥–∏—Ä—É–µ–º–æ–π —Ü–µ–ª—å—é –¥–ª—è –±–∏–∑–Ω–µ—Å–∞: —Å–æ–∫—Ä–∞—Ç–∏—Ç—å –Ω–µ–∑–∞–ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø—Ä–æ—Å—Ç–æ–∏ –Ω–∞ X% (—Ü–µ–ª–µ–≤–æ–µ), –º–∞–∫—Å–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å lead time –¥–ª—è —Ç–µ—Ö–æ–±—Å–ª—É–∂–∏–≤–∞–Ω–∏—è.



---

4) Preprocessing (–∫—Ä–∏—Ç–∏—á–Ω–æ)

–°–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—è —Ç–∞–π–º—à—Ç–∞–º–ø–æ–≤, —Ä–µ—Å–µ–º–ø–ª–∏–Ω–≥ –¥–æ –µ–¥–∏–Ω–æ–π —á–∞—Å—Ç–æ—Ç—ã (–Ω–∞–ø—Ä. 1 –º–∏–Ω).

–†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –ø–æ machine_id ‚Äî –≤—Å–µ –∞–≥—Ä–µ–≥–∞—Ü–∏–∏ –≤–Ω—É—Ç—Ä–∏ –º–∞—à–∏–Ω—ã.

–î–µ—Ç—Ä–µ–Ω–¥–∏–Ω–≥ / —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è —à—É–º–æ–≤ (low-pass / median) –¥–ª—è –≤–∏–±—Ä–∞—Ü–∏–∏.

–ú–∞—Ä–∫–∏—Ä–æ–≤–∫–∞ –ø—Ä–æ–ø—É—Å–∫–æ–≤ (flag) ‚Äî –∏—Ö –Ω–∞–ª–∏—á–∏–µ –≤–∞–∂–Ω–æ.

Windowing: —Å–∫–æ–ª—å–∑—è—â–∏–µ –æ–∫–Ω–∞ (–Ω–∞–ø—Ä–∏–º–µ—Ä, 30‚Äì300 —Å) –∏ –∞–≥—Ä–µ–≥–∞—Ç—ã ‚Äî mean/std/max/min/slope.

–°–æ–∑–¥–∞–Ω–∏–µ —Ç–∞—Ä–≥–µ—Ç–æ–≤: rul (time to failure) –∏ label_horizon (rul <= H).



---

5) Feature engineering (—Ä–∞–±–æ—Ç–∞—é—â–∏–µ —Ñ–∏—á–∏)

Time-domain: rolling mean/std/last/slope –¥–ª—è vibration/temp/current.

Rate features: d(vibration)/dt, relative change in current.

Frequency features –¥–ª—è –≤–∏–±—Ä–∞—Ü–∏–∏: FFT-peak, band energy, spectral centroid.

Health indexes: RMS vibration, kurtosis (–¥–ª—è –ø–æ–¥—à–∏–ø–Ω–∏–∫–æ–≤).

Usage features: running_hours_since_last_repair, cycles_count.

Aggreg–∞—Ç–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏: %time_above_threshold –∑–∞ –æ–∫–Ω–æ, peaks_count.

–õ–∞—Ç–µ–Ω—Ç–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏: bottleneck –∏–∑ –∞–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä–∞ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ).



---

6) –ê–ª–≥–æ—Ä–∏—Ç–º—ã ‚Äî —á—Ç–æ –∏ –ø–æ—á–µ–º—É

Supervised (–µ—Å–ª–∏ –µ—Å—Ç—å –º–µ—Ç–∫–∏):

RUL regression: GradientBoostingRegressor / XGBoost / LightGBM ‚Äî –±—ã—Å—Ç—Ä—ã–π, —É—Å—Ç–æ–π—á–∏–≤—ã–π –∫ —à—É–º—É.

Failure classifier (horizon): RandomForest / XGBoost ‚Äî probability output, –ª–µ–≥–∫–æ –æ–±—ä—è—Å–Ω–∏–º.

Survival analysis: CoxPH (lifelines) ‚Äî –µ—Å–ª–∏ –≤–∞–∂–Ω–∞ –º–æ–¥–µ–ª—å –≤—Ä–µ–º–µ–Ω–∏ –¥–æ —Å–æ–±—ã—Ç–∏—è –∏ —Ü–µ–Ω–∏—Ç—Å—è —Ü–µ–Ω–∑—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –æ—Ü–µ–Ω–∫–∞.


Unsupervised / semi-supervised:

Autoencoder (LSTM/Conv1D) –¥–ª—è reconstruction error ‚Üí –ø—Ä–µ–¥–≤–µ—Å—Ç–Ω–∏–∫–∏ –¥–µ–≥—Ä–∞–¥–∞—Ü–∏–∏.

One-Class / IsolationForest –ø—Ä–∏ –º–∞–ª–æ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–µ –æ—Ç–∫–∞–∑–æ–≤.


–û–Ω–ª–∞–π–Ω: –º–æ–¥–µ–ª–∏ LightGBM/XGBoost –æ–Ω–ª–∞–π–Ω-—Å–∫–æ—Ä—è—Ç—Å—è —á–µ—Ä–µ–∑ FastAPI; drift detection ‚Äî —Ç—Ä–∏–≥–≥–µ—Ä retrain.


---

7) –í–∞–ª–∏–¥–∞—Ü–∏—è –∏ –º–µ—Ç—Ä–∏–∫–∏

Regression (RUL): MAE (—á–∞—Å—ã/—Ü–∏–∫–ª—ã), RMSE, R¬≤.
Classification (failure-in-H): ROC-AUC, PR-AUC, Precision@K, Recall@lead_time, F1.
Operational metrics: Lead time (median/mean) ‚Äî —Å–∫–æ–ª—å–∫–æ –∑–∞—Ä–∞–Ω–µ–µ –º–æ–¥–µ–ª—å –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–∞–µ—Ç; reduction in MTTR/MTBF; reduction of unplanned downtime (—á–∞—Å—ã/–≥–æ–¥).
–í–∞–ª–∏–¥–∞—Ü–∏—è: time-aware split (train –Ω–∞ —Ä–∞–Ω–Ω–∏—Ö –¥–∞–Ω–Ω—ã—Ö, test ‚Äî –Ω–∞ –Ω–æ–≤—ã—Ö), CV: grouped by machine / rolling windows. Shadow mode ‚Äî 2‚Äì6 –Ω–µ–¥–µ–ª—å –¥–ª—è –æ—Ü–µ–Ω–∫–∏.


---

8) Deployment & MLOps (–ø—Ä–∞–∫—Ç–∏—á–Ω–æ)

Ingest ‚Üí Kafka/MQTT (–ø—Ä–∏ —Å—Ç—Ä–∏–º–µ) –∏–ª–∏ —Ñ–∞–π–ª–æ–≤–∞—è –≤—ã–≥—Ä—É–∑–∫–∞ –≤ S3.

Preproc ‚Üí Spark/Flink / lightweight service.

Score API ‚Üí FastAPI (–º–æ–¥–µ–ª—å), —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –ø—Ä–æ–≥–Ω–æ–∑—ã –≤ TSDB (Influx) + –≤ S3 CSV.

Alerts ‚Üí rule engine ‚Üí SCADA/Grafana/Slack/SMS.

Model registry ‚Üí MLflow; –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ drift ‚Üí –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π retrain pipeline.

Audit: –ª–æ–≥ –≤–µ—Ä—Å–∏–π –º–æ–¥–µ–ª–∏, –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö, SHAP-–æ—Ç—á—ë—Ç–æ–≤ (–¥–ª—è —Ä–µ–≥—É–ª—è—Ç–æ—Ä–∞ –∏ –∏–Ω–∂–µ–Ω–µ—Ä–æ–≤).



---

9) Explainability / Operator UX

Top features (per-prediction SHAP) ‚Äî –ø–æ—á–µ–º—É –∏–º–µ–Ω–Ω–æ —Ä–∏—Å–∫ –ø–æ–¥–Ω—è–ª—Å—è.

Health dashboard: –¥–ª—è –∫–∞–∂–¥–æ–π –º–∞—à–∏–Ω—ã ‚Äî P(failure), RUL, top-3 contributing sensors, recommended action (inspect bearing / schedule replacement).

Ticketing: auto-create maintenance ticket —Å priority –∏ predicted window.



---

10) –†–∏—Å–∫–∏ –∏ –º–∏—Ç–∏–≥–µ–π—Ç—ã

False positives ‚Üí ensemble + dynamic threshold + confirmation rule (–ø—Ä–æ–≤–µ—Ä–æ—á–Ω—ã–π —Å–µ–Ω—Å–æ—Ä / –≤–∑—è—Ç—å –ø—Ä–æ–±—É).

Data drift ‚Üí drift detection triggers retrain.

Sensor failure ‚â† machine failure ‚Üí sensor health channel + redundancy.

Operator distrust ‚Üí shadow mode + –æ–±—É—á–µ–Ω–∏–µ + explainable tiles.

Regulatory trace ‚Üí audit trail: –º–æ–¥–µ–ª—å, –≤–µ—Ä—Å–∏—è, –≤—Ö–æ–¥, –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ.



---

11) ROI & –±–∏–∑–Ω–µ—Å-—ç—Ñ—Ñ–µ–∫—Ç (–∫—Ä–∞—Ç–∫–æ, —á–∏—Å–ª–∞)

(–æ—Ü–µ–Ω–∫–∏ –æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–æ—á–Ω—ã–µ; –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ —Ä–µ–∞–ª—å–Ω—ã–µ —Ü–∏—Ñ—Ä—ã ChemSolutions –¥–ª—è —Ç–æ—á–Ω–æ—Å—Ç–∏)

–£–º–µ–Ω—å—à–µ–Ω–∏–µ –Ω–µ–∑–∞–ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –ø—Ä–æ—Å—Ç–æ–µ–≤: ‚àí25‚Äì40% ‚Üí —ç–∫–æ–Ω–æ–º–∏—è $2‚Äì5M/–≥–æ–¥.

–°–æ–∫—Ä–∞—â–µ–Ω–∏–µ –∞–≤–∞—Ä–∏–π–Ω—ã—Ö —Ä–µ–º–æ–Ω—Ç–æ–≤ –∏ –∑–∞–ø–∞—Å–æ–≤ –∑–∞–ø—á–∞—Å—Ç–µ–π: ‚àí10‚Äì20% –∑–∞—Ç—Ä–∞—Ç –Ω–∞ –∑–∞–ø—á–∞—Å—Ç–∏.

–£–≤–µ–ª–∏—á–µ–Ω–∏–µ OEE –∏ On-Time Delivery (–ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ –∫–æ–Ω—Ç—Ä–∞–∫—Ç–æ–≤) ‚Üí –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –≤—ã—Ä—É—á–∫–∞.

–¢–∏–ø–∏—á–Ω—ã–π CAPEX –Ω–∞ PdM-–ø–∏–ª–æ—Ç: $80‚Äì200k; –æ–∫—É–ø–∞–µ–º–æ—Å—Ç—å ‚Äî 3‚Äì9 –º–µ—Å—è—Ü–µ–≤ –ø—Ä–∏ –º–∞—Å—à—Ç–∞–±–µ –ø—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤–∞ —Å—Ä–µ–¥–Ω–µ–≥–æ —Ä–∞–∑–º–µ—Ä–∞.



---




---

12) –ö–∞–∫ —ç—Ç–æ –ø–æ–≤—ã—à–∞–µ—Ç –æ—Ü–µ–Ω–∫–∏ –∂—é—Ä–∏ (mapping)

–ê–∫—Ç—É–∞–ª—å–Ω–æ—Å—Ç—å: —É–º–µ–Ω—å—à–∞–µ—Ç –¥–æ—Ä–æ–≥–æ—Å—Ç–æ—è—â–∏–µ –Ω–µ–∑–∞–ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø—Ä–æ—Å—Ç–æ–∏ ‚Üí –ø—Ä—è–º–æ–π –±–∏–∑–Ω–µ—Å-–±–æ–ª–ª.

–ò–Ω–Ω–æ–≤–∞—Ü–∏–∏: –≥–∏–±—Ä–∏–¥ supervised+autosensing, survival modeling –∏ explainable outputs.

–ë–∏–∑–Ω–µ—Å-—Ä–µ–∞–ª–∏–∑—É–µ–º–æ—Å—Ç—å: –º–æ–¥—É–ª—å–Ω–æ–µ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ SCADA / CMMS ‚Äî –±—ã—Å—Ç—Ä—ã–π –ø–∏–ª–æ—Ç.

–¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–π –≤–∫–ª–∞–¥: RUL + probabilistic failure, real-time scoring, drift detection.

–ü—Ä–µ–∑–µ–Ω—Ç–∞—Ü–∏—è: –≥—Ä–∞—Ñ–∏–∫–∏ (lead time, RUL scatter, ROC) ‚Äî –ø–æ–Ω—è—Ç–Ω—ã–µ —Å—É–¥—å–µ.



---

14) MVP-–∫–æ–¥: contour_c.py (–≤—Å—Ç–∞–≤–ª—è–π—Ç–µ –≤ —Ä–µ–ø–æ)

> –ò–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏:

1. –ü–æ–ª–æ–∂–∏—Ç–µ contour_c.py –≤ –ø–∞–ø–∫—É —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è.


2. pip install -r requirements.txt (requirements –≤–Ω–∏–∑—É).


3. –ó–∞–ø—É—Å—Ç–∏—Ç–µ python contour_c.py.


4. –ï—Å–ª–∏ –µ—Å—Ç—å data_C.csv ‚Äî —Å–∫—Ä–∏–ø—Ç –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Ä–µ–∞–ª—å–Ω—É—é —Ç–∞–±–ª–∏—Ü—É (—Ñ–æ—Ä–º–∞—Ç –æ–ø–∏—Å–∞–Ω –≤ –∫–æ–¥–µ). –ï—Å–ª–∏ –Ω–µ—Ç ‚Äî –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Å–∏–Ω—Ç–µ—Ç–∏–∫—É.


5. –°–∫—Ä–∏–ø—Ç —Å–æ—Ö—Ä–∞–Ω–∏—Ç PNG –∏ CSV ‚Äî –∏—Ö –≤—Å—Ç–∞–≤–ª—è–µ—Ç–µ –≤ —Å–ª–∞–π–¥—ã/README.





# contour_c.py
# Python 3.8+
# pip install numpy pandas matplotlib scikit-learn seaborn joblib

import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.ensemble import GradientBoostingRegressor, RandomForestClassifier
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, roc_auc_score, precision_score, recall_score
from sklearn.model_selection import train_test_split
from joblib import dump

sns.set(style="whitegrid", rc={"figure.figsize": (10,5)})

def generate_synthetic_pd_data(n_machines=8, seed=42):
    np.random.seed(seed)
    rows = []
    for mid in range(n_machines):
        life = np.random.randint(400, 1200)  # cycles until failure for this machine
        baseline_vib = 0.5 + np.random.rand()*0.5
        baseline_temp = 50 + np.random.rand()*10
        baseline_cur  = 5 + np.random.rand()*2
        for t in range(life):
            # slow degradation + noise
            vib = baseline_vib + 0.0008*t + np.random.normal(0, 0.05) + 0.01*np.sin(t/10)
            temp = baseline_temp + 0.0006*t + np.random.normal(0, 0.2)
            cur  = baseline_cur + 0.0004*t + np.random.normal(0, 0.05)
            rows.append((mid, t, vib, temp, cur, life - t))
    df = pd.DataFrame(rows, columns=["machine_id","t","vibration","temperature","current","rul"])
    # failure label for horizon H (we'll compute later)
    return df

def fe_engineer(df, window=5):
    df = df.sort_values(["machine_id","t"]).reset_index(drop=True)
    df_grouped = []
    for mid, g in df.groupby("machine_id"):
        g = g.copy()
        for col in ["vibration","temperature","current"]:
            g[f"{col}_mean"] = g[col].rolling(window, min_periods=1).mean()
            g[f"{col}_std"]  = g[col].rolling(window, min_periods=1).std().fillna(0)
            g[f"{col}_slope"] = g[col].diff().rolling(window, min_periods=1).mean().fillna(0)
            g[f"{col}_last"] = g[col]
        g["running_hours"] = g["t"]
        df_grouped.append(g)
    return pd.concat(df_grouped).reset_index(drop=True)

def prepare_labels(df, horizon=50):
    # classification label: will fail within horizon?
    df["label_horizon"] = (df["rul"] <= horizon).astype(int)
    return df

def train_models(df, horizon=50):
    features = [c for c in df.columns if any(x in c for x in ["vibration_","temperature_","current_","running_hours"])]
    # time-aware split: for each machine take first 70% timestamps as train, rest as test
    train_idx = []
    test_idx = []
    for mid, g in df.groupby("machine_id"):
        n = len(g)
        split = int(0.7 * n)
        train_idx.extend(g.index[:split].tolist())
        test_idx.extend(g.index[split:].tolist())
    train = df.loc[train_idx].reset_index(drop=True)
    test  = df.loc[test_idx].reset_index(drop=True)

    X_train = train[features].fillna(0)
    y_rul_train = train["rul"]
    y_clf_train = train["label_horizon"]

    X_test = test[features].fillna(0)
    y_rul_test = test["rul"]
    y_clf_test = test["label_horizon"]

    # Regression: RUL
    reg = GradientBoostingRegressor(n_estimators=200, learning_rate=0.05, max_depth=4, random_state=42)
    reg.fit(X_train, y_rul_train)
    y_rul_pred = reg.predict(X_test)
    mae = mean_absolute_error(y_rul_test, y_rul_pred)
    rmse = mean_squared_error(y_rul_test, y_rul_pred, squared=False)
    r2 = r2_score(y_rul_test, y_rul_pred)

    # Classification: failure within horizon
    clf = RandomForestClassifier(n_estimators=200, max_depth=6, random_state=42)
    clf.fit(X_train, y_clf_train)
    y_prob = clf.predict_proba(X_test)[:,1]
    y_pred = (y_prob >= 0.5).astype(int)
    roc = roc_auc_score(y_clf_test, y_prob)
    prec = precision_score(y_clf_test, y_pred, zero_division=0)
    rec = recall_score(y_clf_test, y_pred, zero_division=0)

    metrics = {
        "rul_mae": mae, "rul_rmse": rmse, "rul_r2": r2,
        "clf_roc_auc": roc, "clf_precision": prec, "clf_recall": rec
    }

    results = {
        "model_reg": reg,
        "model_clf": clf,
        "X_test": X_test, "y_rul_test": y_rul_test, "y_rul_pred": y_rul_pred,
        "y_clf_test": y_clf_test, "y_prob": y_prob,
        "metrics": metrics, "features": features, "test_idx": test.index
    }
    return results

def plots_and_export(df, results, out_prefix="contourC"):
    # RUL scatter
    plt.figure(figsize=(7,5))
    plt.scatter(results["y_rul_test"], results["y_rul_pred"], alpha=0.5)
    m = max(results["y_rul_test"].max(), np.nanmax(results["y_rul_pred"]))
    plt.plot([0,m],[0,m], "r--")
    plt.xlabel("Actual RUL")
    plt.ylabel("Predicted RUL")
    plt.title("Predicted vs Actual RUL")
    plt.tight_layout()
    plt.savefig(f"{out_prefix}_rul_scatter.png")
    plt.close()

    # ROC
    from sklearn.metrics import roc_curve, auc
    fpr, tpr, _ = roc_curve(results["y_clf_test"], results["y_prob"])
    roc_auc = auc(fpr, tpr)
    plt.figure(figsize=(6,5))
    plt.plot(fpr, tpr, label=f"AUC = {roc_auc:.3f}")
    plt.plot([0,1],[0,1],"k--")
    plt.xlabel("FPR")
    plt.ylabel("TPR")
    plt.title("ROC - Failure within horizon")
    plt.legend()
    plt.tight_layout()
    plt.savefig(f"{out_prefix}_roc.png")
    plt.close()

    # feature importances (clf)
    importances = results["model_clf"].feature_importances_
    feat_imp = pd.Series(importances, index=results["features"]).sort_values(ascending=False).head(12)
    plt.figure(figsize=(8,4))
    feat_imp.plot(kind="barh")
    plt.gca().invert_yaxis()
    plt.title("Top feature importances (classifier)")
    plt.tight_layout()
    plt.savefig(f"{out_prefix}_feat_importance.png")
    plt.close()

    # timeline for a sample machine (last machine in dataset)
    sample_mid = df["machine_id"].unique()[-1]
    sample = df[df["machine_id"]==sample_mid].copy()
    # map predictions onto sample times if indices align
    test_idx_map = results["test_idx"].tolist()
    sample_test = sample[sample.index.isin(test_idx_map)]
    if not sample_test.empty:
        sample_test = sample_test.copy()
        sample_test["pred_prob"] = results["y_prob"][-len(sample_test):]
        sample_test["pred_rul"] = results["y_rul_pred"][-len(sample_test):]
        plt.figure(figsize=(12,4))
        plt.plot(sample_test["t"], sample_test["vibration_last"], label="vibration")
        plt.plot(sample_test["t"], sample_test["pred_prob"]*sample_test["vibration_last"].max(), label="pred_prob (scaled)")
        plt.scatter(sample_test["t"][sample_test["label_horizon"]==1], sample_test["vibration_last"][sample_test["label_horizon"]==1], color="red", s=20, label="actual failure window")
        plt.title(f"Machine {sample_mid} timeline (vibration + predicted prob)")
        plt.legend()
        plt.tight_layout()
        plt.savefig(f"{out_prefix}_timeline_machine_{sample_mid}.png")
        plt.close()

    # save CSV results
    df_results = sample_test[["machine_id","t","vibration_last","temperature_last","current_last","pred_prob","pred_rul","label_horizon"]].copy() if not sample_test.empty else pd.DataFrame()
    df_results.to_csv(f"{out_prefix}_sample_timeline.csv", index=False)
    # Save metrics
    with open(f"{out_prefix}_metrics.txt","w") as f:
        for k,v in results["metrics"].items():
            f.write(f"{k}: {v}\n")
    print("Saved plots and metrics files.")

def main():
    # 1) Load data_C.csv if exists, else generate synthetic
    if os.path.exists("data_C.csv"):
        df = pd.read_csv("data_C.csv")
        # Expected columns: machine_id,timestamp,vibration,temperature,current, (optionally failure/time_to_failure)
        # Minimal required: machine_id,t (time or cycle index), vibration, temperature, current
        # If time_to_failure or rul present, use it; else we'll not have supervised labels.
        if "rul" not in df.columns:
            print("data_C.csv found but no 'rul' column ‚Äî supervised RUL won't be available. Consider adding 'rul' or run unsupervised mode.")
    else:
        print("No data_C.csv found ‚Äî generating synthetic dataset...")
        df = generate_synthetic_pd_data(n_machines=8)
    df_fe = fe_engineer(df, window=5)
    df_label = prepare_labels(df_fe, horizon=50)
    # Train models (supervised)
    results = train_models(df_label, horizon=50)
    # Save models
    dump(results["model_reg"], "contourC_reg.joblib")
    dump(results["model_clf"], "contourC_clf.joblib")
    # Attach some columns for timeline plotting (last/mean etc.)
    # rename last cols for readability
    for c in ["vibration_last","temperature_last","current_last"]:
        if c not in df_label.columns:
            # if not exist, map from base columns
            base = c.split("_")[0]
            df_label[c] = df_label[base]
    plots_and_export(df_label, results, out_prefix="contourC")
    # Save full test-results CSV (merge indices)
    test_idx = results["test_idx"]
    test_df = df_label.loc[test_idx].copy()
    test_df["pred_rul"] = results["y_rul_pred"]
    test_df["pred_prob"] = results["y_prob"]
    test_df.to_csv("contourC_results.csv", index=False)
    print("Saved contourC_results.csv")
    print("Metrics:", results["metrics"])
    print("Models saved: contourC_reg.joblib, contourC_clf.joblib")

if __name__ == "__main__":
    main()


---

15) requirements.txt (–º–∏–Ω–∏–º—É–º)

numpy
pandas
matplotlib
seaborn
scikit-learn
joblib


---



---

16) –ö–æ—Ä–æ—Ç–∫–æ ‚Äî —á—Ç–æ –Ω—É–∂–Ω–æ –ø–æ–∫–∞–∑–∞—Ç—å –∂—é—Ä–∏ (—Å–ª–∞–π–¥—ã/README)

RUL scatter (MAE, RMSE) ‚Äî –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç —Ç–æ—á–Ω–æ—Å—Ç—å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —Å—Ä–æ–∫–∞ –∂–∏–∑–Ω–∏.

ROC + Precision/Recall ‚Äî –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–∞—Ç—å –æ–± –æ—Ç–∫–∞–∑–µ.

Timeline sample ‚Äî –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç lead time –∏ –∫–∞–∫ –≤–∏–∑—É–∞–ª—å–Ω–æ –æ–ø–µ—Ä–∞—Ç–æ—Ä –≤–∏–¥–∏—Ç –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ.

Feature importance & SHAP (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ) ‚Äî –æ–±—ä—è—Å–Ω–∏–º–æ—Å—Ç—å.

Business slide ‚Äî —Å–∫–æ–ª—å–∫–æ —á–∞—Å–æ–≤ –ø—Ä–æ—Å—Ç–æ–µ–≤ —Å—ç–∫–æ–Ω–æ–º–ª–µ–Ω–æ, ROI (–ø—Ä–∏–º–µ—Ä: ‚àí25% –Ω–µ–∑–∞–ø–ª–∞–Ω. –ø—Ä–æ—Å—Ç–æ–µ–≤ ‚Üí $2‚Äì5M/–≥–æ–¥).













üõ† –ò–Ω—Å—Ç—Ä—É–∫—Ü–∏—è –¥–ª—è –∂—é—Ä–∏

–ß—Ç–æ–±—ã –≤–æ—Å–ø—Ä–æ–∏–∑–≤–µ—Å—Ç–∏ —Ä–∞–±–æ—Ç—É –ø—Ä–æ—Ç–æ—Ç–∏–ø–∞:

1. –û—Ç–∫—Ä–æ–π—Ç–µ Google Colab

–ü–µ—Ä–µ–π–¥–∏—Ç–µ –Ω–∞ https://colab.research.google.com/.

–ù–∞–∂–º–∏—Ç–µ File ‚Üí New Notebook.



2. –°–∫–æ–ø–∏—Ä—É–π—Ç–µ –∫–æ–¥

–í–æ–∑—å–º–∏—Ç–µ –±–ª–æ–∫ –∫–æ–¥–∞ –∏–∑ —ç—Ç–æ–≥–æ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è (–≤ README —É–∂–µ –≤—Å—Ç–∞–≤–ª–µ–Ω).

–í—Å—Ç–∞–≤—å—Ç–µ –µ–≥–æ –≤ –ø–µ—Ä–≤—É—é —è—á–µ–π–∫—É Colab.



3. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏

–ó–∞–ø—É—Å—Ç–∏—Ç–µ –∫–æ–º–∞–Ω–¥—É (–ø–µ—Ä–≤–∞—è —Å—Ç—Ä–æ–∫–∞ –∫–æ–¥–∞):

!pip install xgboost shap scikit-learn pandas matplotlib



4. –ó–∞–ø—É—Å—Ç–∏—Ç–µ –≤–µ—Å—å –∫–æ–¥ –ø–æ –ø–æ—Ä—è–¥–∫—É

–ù–∞–∂–º–∏—Ç–µ Runtime ‚Üí Run all.

–ü–æ—Å–ª–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –ø–æ—è–≤—è—Ç—Å—è:

–º–µ—Ç—Ä–∏–∫–∏ –º–æ–¥–µ–ª–∏ (ROC-AUC, MAE –∏ –¥—Ä.);

–≥—Ä–∞—Ñ–∏–∫–∏ (SHAP summary plot, predicted vs real);

–ø—Ä–æ—Å—Ç—ã–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ (¬´—É–º–µ–Ω—å—à–∏—Ç–µ temp –Ω–∞ 1‚ÑÉ ‚Üí +Œî –∫ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –∫–∞—á–µ—Å—Ç–≤–∞¬ª).




5. –ß—Ç–æ —Å–º–æ—Ç—Ä–µ—Ç—å –≤ –≤—ã–≤–æ–¥–µ

–ú–µ—Ç—Ä–∏–∫–∏ ‚Üí –∫–∞—á–µ—Å—Ç–≤–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è.

–ì—Ä–∞—Ñ–∏–∫–∏ SHAP ‚Üí –∫–∞–∫–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –≤–ª–∏—è—é—Ç –Ω–∞ –ø—Ä–æ–≥–Ω–æ–∑.

–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ ‚Üí –ø—Ä–∏–º–µ—Ä—ã –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö —Å–æ–≤–µ—Ç–æ–≤ –¥–ª—è –æ–ø–µ—Ä–∞—Ç–æ—Ä–∞.





---

‚ö° –ü—Ä–∏–º–µ—á–∞–Ω–∏–µ: –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –º—ã –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ, –Ω–æ –∫–æ–¥ –ø–æ–ª–Ω–æ—Å—Ç—å—é –≥–æ—Ç–æ–≤ —Ä–∞–±–æ—Ç–∞—Ç—å –∏ —Å —Ä–µ–∞–ª—å–Ω—ã–º–∏ –¥–∞—Ç–∞—Å–µ—Ç–∞–º–∏ (–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –∑–∞–º–µ–Ω–∏—Ç—å batches_aggregated.csv).



